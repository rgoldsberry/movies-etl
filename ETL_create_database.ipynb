{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from config import db_password\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Add the clean movie function that takes in the argument, \"movie\".\n",
    "def clean_movie(movie):\n",
    "    movie = dict(movie) #create a non-destructive copy\n",
    "    \n",
    "    #take all alternative titles, collapse to one column\n",
    "    alt_titles = {}\n",
    "\n",
    "    #loop through alt title names\n",
    "    for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                'Mandarin','McCune–Reischauer','Original title','Polish',\n",
    "                'Revised Romanization','Romanized','Russian',\n",
    "                'Simplified','Traditional','Yiddish']:\n",
    "\n",
    "        #if any alt titles exist, add them to a dictionary, put that dictionary in movie list\n",
    "        if key in movie:\n",
    "            alt_titles[key] = movie[key]\n",
    "            movie.pop(key)\n",
    "        \n",
    "        if len(alt_titles) > 0:\n",
    "            movie['alt_titles'] = alt_titles\n",
    "    \n",
    "    #function to condense similar columns into one\n",
    "    def change_column_name(old_name, new_name):\n",
    "        if old_name in movie:\n",
    "            movie[new_name] = movie.pop(old_name)\n",
    "\n",
    "    #run above function on similar columns\n",
    "    #see ColumnChanges.xlsx for work on deciding these columns\n",
    "    change_column_name('Adaptation by', 'Writer(s)')\n",
    "    change_column_name('Country of origin', 'Country')\n",
    "    change_column_name('Directed by', 'Director')\n",
    "    change_column_name('Distributed by', 'Distributor')\n",
    "    change_column_name('Edited by', 'Editor(s)')\n",
    "    change_column_name('Length', 'Running time')\n",
    "    change_column_name('Music by', 'Composer(s)')\n",
    "    change_column_name('Original language(s)', 'Language')\n",
    "    change_column_name('Original release', 'Release date')\n",
    "    change_column_name('Produced by', 'Producer(s)')\n",
    "    change_column_name('Producer', 'Producer(s)')\n",
    "    change_column_name('Productioncompanies ', 'Production company(s)')\n",
    "    change_column_name('Productioncompany ', 'Production company(s)')\n",
    "    change_column_name('Released', 'Release date')\n",
    "    change_column_name('Screen story by', 'Writer(s)')\n",
    "    change_column_name('Screenplay by', 'Writer(s)')\n",
    "    change_column_name('Story by', 'Writer(s)')\n",
    "    change_column_name('Voices of', 'Starring')\n",
    "    change_column_name('Written by', 'Writer(s)')\n",
    "\n",
    "\n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Add the function that takes in three arguments;\n",
    "# Wikipedia data, Kaggle metadata, and MovieLens rating data (from Kaggle)\n",
    "\n",
    "def extract_transform_load():\n",
    "    # Read in the kaggle metadata and MovieLens ratings CSV files as Pandas DataFrames.\n",
    "    kaggle_metadata = pd.read_csv(kaggle_file, low_memory=False)\n",
    "    ratings = pd.read_csv(ratings_file)\n",
    "\n",
    "    # Open the read the Wikipedia data JSON file.\n",
    "    with open(wiki_file, mode = 'r') as file:\n",
    "        wiki_movies_raw = json.load(file)\n",
    "    \n",
    "    # Write a list comprehension to filter out TV shows.\n",
    "    wiki_movies = [movie for movie in wiki_movies_raw\n",
    "                if ('Director' in movie or 'Directed by' in movie)\n",
    "                and 'imdb_link' in movie\n",
    "                and 'No. of episodes' not in movie]\n",
    "    \n",
    "\n",
    "    # Write a list comprehension to iterate through the cleaned wiki movies list\n",
    "    # and call the clean_movie function on each movie.\n",
    "    clean_movies = [clean_movie(movie) for movie in wiki_movies]\n",
    "    \n",
    "\n",
    "    # Read in the cleaned movies list from Step 4 as a DataFrame.\n",
    "    wiki_movies_df = pd.DataFrame(clean_movies)\n",
    "\n",
    "\n",
    "    # Write a try-except block to catch errors while extracting the IMDb ID using a regular expression string and\n",
    "    #  dropping any imdb_id duplicates. If there is an error, capture and print the exception.\n",
    "    try:\n",
    "        wiki_movies_df[\"imdb_id\"] = wiki_movies_df[\"imdb_link\"].str.extract(r'(tt\\d{7})')\n",
    "        wiki_movies_df.drop_duplicates(subset='imdb_id', inplace=True)\n",
    "    except:\n",
    "        print(f'Issue with {wiki_movies_df[\"title\"]}')\n",
    "\n",
    "    \n",
    "    #  Write a list comprehension to keep the columns that don't have null values from the wiki_movies_df DataFrame.\n",
    "    wiki_columns_to_keep = [column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9]\n",
    "    wiki_movies_df = wiki_movies_df[wiki_columns_to_keep]\n",
    "    \n",
    "\n",
    "    # Create a variable that will hold the non-null values from the “Box office” column.\n",
    "    box_office = wiki_movies_df['Box office'].dropna()\n",
    "    \n",
    "    # Convert the box office data created in Step 8 to string values using the lambda and join functions.\n",
    "    box_office = box_office.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    #condense ranges\n",
    "    box_office = box_office.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "    \n",
    "\n",
    "    # Write a regular expression to match the six elements of \"form_one\" of the box office data.\n",
    "    # $ 53 [mb]illion\n",
    "    form_one = r\"\\$\\s*\\d+\\.?\\d*\\s*[mb]ill?i?on\"\n",
    "       \n",
    "    # Write a regular expression to match the three elements of \"form_two\" of the box office data.\n",
    "    # $ 123,500,000\n",
    "    form_two = r\"\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illi?on)\"\n",
    "    \n",
    "    # Add the parse_dollars function.\n",
    "    def parse_dollars(s):\n",
    "        # if s is not a string, return NaN\n",
    "        if type(s) != str:\n",
    "            return np.nan\n",
    "\n",
    "        # if input is of the form $###.# million\n",
    "        if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*mill?i?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "            # remove dollar sign and \" million\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "            # convert to float and multiply by a million\n",
    "            value = float(s) * 10**6\n",
    "\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # if input is of the form $###.# billion\n",
    "        elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*bill?i?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "            # remove dollar sign and \" billion\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "            # convert to float and multiply by a billion\n",
    "            value = float(s) * 10**9\n",
    "\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # if input is of the form $###,###,###\n",
    "        elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illi?on)', s, flags=re.IGNORECASE):\n",
    "\n",
    "            # remove dollar sign and commas\n",
    "            s = re.sub('\\$|,','', s)\n",
    "\n",
    "            # convert to float\n",
    "            value = float(s)\n",
    "\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # otherwise, return NaN\n",
    "        else:\n",
    "            return np.nan\n",
    "    \n",
    "        \n",
    "    # Clean the box office column in the wiki_movies_df DataFrame.\n",
    "    wiki_movies_df['box_office'] = box_office.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    wiki_movies_df.drop('Box office', axis = 1, inplace=True)\n",
    "\n",
    "    \n",
    "    # Clean the budget column in the wiki_movies_df DataFrame.\n",
    "    \n",
    "    #create budget series to work with\n",
    "    budget = wiki_movies_df['Budget'].dropna()\n",
    "    budget = budget.map(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    #condense ranges\n",
    "    budget = budget.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "\n",
    "    #get rid of citations\n",
    "    budget = budget.str.replace(r'\\[\\d+\\]\\s*', '')\n",
    "\n",
    "    #apply the parse dollars function and drop old column\n",
    "    wiki_movies_df['budget'] = budget.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    # wiki_movies_df.drop('Budget', axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "    # Clean the release date column in the wiki_movies_df DataFrame.\n",
    "    #get release date series to work with\n",
    "    release_date = wiki_movies_df['Release date'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    # define date forms\n",
    "    # ( 1994-01-02 )\n",
    "    date_form_one = r'\\(\\s\\d{4}.+\\d{1,2}.+\\d{1,2}\\s\\)'\n",
    "    # January 2, 1994 \n",
    "    date_form_two = r'[a-zA-Z]{3,9}\\s*\\d{1,2},?\\s*\\d{4}'\n",
    "    # January 1994\n",
    "    date_form_three = r'[a-zA-Z]{3,9}\\s*\\d{4}'\n",
    "    # 1994\n",
    "    date_form_four = r'\\d{4}'\n",
    "\n",
    "    #put the parsed release dates in the dataframe, drop the old one\n",
    "    wiki_movies_df['release_date'] = pd.to_datetime(release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})', \n",
    "                                                    flags=re.IGNORECASE)[0], infer_datetime_format=True)\n",
    "\n",
    "    # wiki_movies_df.drop('Release date', axis=1, inplace=True)\n",
    "\n",
    "    # Clean the running time column in the wiki_movies_df DataFrame.\n",
    "    # get running time series to work with\n",
    "    running_time = wiki_movies_df['Running time'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "\n",
    "    # two main length formats: 124 m[inutes] or 2 h[ours] 35 [minutes]\n",
    "    # extract the two main length formats, convert to number\n",
    "    running_time_extract = running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')\n",
    "    running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)\n",
    "\n",
    "    #convert both capture groups to minutes total, drop the old column\n",
    "    wiki_movies_df['running_time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)\n",
    "    wiki_movies_df.drop('Running time', axis=1, inplace=True)\n",
    "         \n",
    "    # Clean the Kaggle metadata.\n",
    "    \n",
    "    # keep only non-adult films and drop the column\n",
    "    kaggle_metadata = kaggle_metadata[kaggle_metadata['adult'] == 'False'].drop('adult',axis='columns')\n",
    "\n",
    "    # convert video col to bool\n",
    "    kaggle_metadata['video'] = kaggle_metadata['video'] == True\n",
    "\n",
    "    # work on numeric columns\n",
    "    kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "    kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "    kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise')\n",
    "\n",
    "    # convert release date to datetime\n",
    "    kaggle_metadata['release_date'] = pd.to_datetime(kaggle_metadata['release_date'])\n",
    "\n",
    "    # Merged the two DataFrames into the movies DataFrame.\n",
    "    movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "\n",
    "    # Drop unnecessary columns from the merged DataFrame.\n",
    "    # Wiki                     Movielens                Resolution\n",
    "    #--------------------------------------------------------------------------\n",
    "    # title_wiki               title_kaggle             drop wikipedia titles col\n",
    "    # running_time             runtime                  keep kaggle, fill in 0s from wikipedia\n",
    "    # budget_wiki              budget_kaggle            keep kaggle, fill in 0s from wikipedia\n",
    "    # box_office               revenue                  keep kaggle, fill in 0s from wikipedia\n",
    "    # release_date_wiki        release_date_kaggle      drop wikipedia release date col\n",
    "    # Language                 original_language        drop wikipedia langauage col\n",
    "    # Production company(s)    production_companies     drop wikipedia production company(s) col\n",
    "\n",
    "    movies_df.drop(columns=['title_wiki','release_date_wiki','Language','Production company(s)'], inplace=True)\n",
    "\n",
    "\n",
    "    # Add in the function to fill in the missing Kaggle data.\n",
    "    \n",
    "    def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "        \n",
    "        #infill the 0s\n",
    "        df[kaggle_column] = df.apply(lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column], axis = 1)\n",
    "\n",
    "        #drop the wiki column\n",
    "        df.drop(columns = wiki_column, inplace = True)\n",
    "\n",
    "\n",
    "    # Call the function in Step 5 with the DataFrame and columns as the arguments.\n",
    "    fill_missing_kaggle_data(movies_df, 'runtime', 'running_time')\n",
    "    fill_missing_kaggle_data(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "    fill_missing_kaggle_data(movies_df, 'revenue', 'box_office')\n",
    "\n",
    "\n",
    "    # Filter the movies DataFrame for specific columns.\n",
    "    movies_df.drop(columns=['video'], inplace=True)\n",
    "    movies_df = movies_df.loc[:, ['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "                       'runtime','budget_kaggle','revenue','release_date_kaggle','popularity','vote_average','vote_count',\n",
    "                       'genres','original_language','overview','spoken_languages','Country',\n",
    "                       'production_companies','production_countries','Distributor',\n",
    "                       'Producer(s)','Director','Starring','Cinematography','Editor(s)','Writer(s)','Composer(s)','Based on'\n",
    "                      ]]\n",
    "\n",
    "\n",
    "    # Rename the columns in the movies DataFrame.\n",
    "    movies_df.rename({'id':'kaggle_id',\n",
    "                  'title_kaggle':'title',\n",
    "                  'url':'wikipedia_url',\n",
    "                  'budget_kaggle':'budget',\n",
    "                  'release_date_kaggle':'release_date',\n",
    "                  'Country':'country',\n",
    "                  'Distributor':'distributor',\n",
    "                  'Producer(s)':'producers',\n",
    "                  'Director':'director',\n",
    "                  'Starring':'starring',\n",
    "                  'Cinematography':'cinematography',\n",
    "                  'Editor(s)':'editors',\n",
    "                  'Writer(s)':'writers',\n",
    "                  'Composer(s)':'composers',\n",
    "                  'Based on':'based_on'\n",
    "                 }, axis='columns', inplace=True)\n",
    "\n",
    "\n",
    "    # Transform and merge the ratings DataFrame.\n",
    "    # convert timestamp to datetime\n",
    "    # ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s', origin='unix')\n",
    "\n",
    "    # create ratings pivot table, indexed by movie id\n",
    "    ratings_grouped = ratings.groupby(['movieId', 'rating'], as_index=False).count() \\\n",
    "                         .rename({'userId' : 'Count'}, axis = 1) \\\n",
    "                         .pivot(index = 'movieId', columns = 'rating', values = 'Count')\n",
    "\n",
    "    # make columns more descriptive\n",
    "    ratings_grouped.columns = ['rating_' + str(col) for col in ratings_grouped.columns]\n",
    "\n",
    "    # merge the ratings df into the movies df\n",
    "    movies_with_ratings_df = pd.merge(movies_df, ratings_grouped, how = 'left', left_on='kaggle_id', right_index = True)\n",
    "    movies_with_ratings_df[ratings_grouped.columns] = movies_with_ratings_df[ratings_grouped.columns].fillna(0)\n",
    "\n",
    "    # create connection to postgres\n",
    "    db_string = f\"postgresql://postgres:{db_password}@127.0.0.1:5432/movie_data\"\n",
    "\n",
    "    #create engine\n",
    "    engine = create_engine(db_string)\n",
    "\n",
    "    #send movies to postgres\n",
    "    movies_df.to_sql(name = 'movies', con = engine, if_exists='replace')\n",
    "\n",
    "    # create a variable for the number of rows imported, and time started\n",
    "    rows_imported = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # loading raw ratings to send to postgres\n",
    "    for data in pd.read_csv(f'{file_dir}ratings.csv', chunksize=1000000):\n",
    "\n",
    "        \n",
    "        print(f'importing rows {rows_imported: ,} to {rows_imported + len(data): ,}...', end='')\n",
    "        data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "        rows_imported += len(data)\n",
    "\n",
    "        # print that the rows have finished importing\n",
    "        print(f'Done. {round(time.time() - start_time, 2)} seconds have elapsed')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the path to your file directory and variables for the three files.\n",
    "file_dir = 'Data/'\n",
    "# The Wikipedia data\n",
    "wiki_file = f'{file_dir}/wikipedia-movies.json'\n",
    "# The Kaggle metadata\n",
    "kaggle_file = f'{file_dir}/movies_metadata.csv'\n",
    "# The MovieLens rating data.\n",
    "ratings_file = f'{file_dir}/ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryang\\anaconda3\\envs\\PyhtonData\\lib\\site-packages\\ipykernel_launcher.py:123: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing rows  0 to  1,000,000...Done. 16.03 seconds have elapsed\n",
      "importing rows  1,000,000 to  2,000,000...Done. 31.44 seconds have elapsed\n",
      "importing rows  2,000,000 to  3,000,000...Done. 47.2 seconds have elapsed\n",
      "importing rows  3,000,000 to  4,000,000...Done. 62.6 seconds have elapsed\n",
      "importing rows  4,000,000 to  5,000,000...Done. 77.92 seconds have elapsed\n",
      "importing rows  5,000,000 to  6,000,000...Done. 93.6 seconds have elapsed\n",
      "importing rows  6,000,000 to  7,000,000...Done. 109.25 seconds have elapsed\n",
      "importing rows  7,000,000 to  8,000,000...Done. 124.7 seconds have elapsed\n",
      "importing rows  8,000,000 to  9,000,000...Done. 140.1 seconds have elapsed\n",
      "importing rows  9,000,000 to  10,000,000...Done. 155.65 seconds have elapsed\n",
      "importing rows  10,000,000 to  11,000,000...Done. 171.53 seconds have elapsed\n",
      "importing rows  11,000,000 to  12,000,000...Done. 186.85 seconds have elapsed\n",
      "importing rows  12,000,000 to  13,000,000...Done. 202.41 seconds have elapsed\n",
      "importing rows  13,000,000 to  14,000,000...Done. 218.29 seconds have elapsed\n",
      "importing rows  14,000,000 to  15,000,000...Done. 233.57 seconds have elapsed\n",
      "importing rows  15,000,000 to  16,000,000...Done. 249.09 seconds have elapsed\n",
      "importing rows  16,000,000 to  17,000,000...Done. 264.87 seconds have elapsed\n",
      "importing rows  17,000,000 to  18,000,000...Done. 280.06 seconds have elapsed\n",
      "importing rows  18,000,000 to  19,000,000...Done. 295.73 seconds have elapsed\n",
      "importing rows  19,000,000 to  20,000,000...Done. 311.1 seconds have elapsed\n",
      "importing rows  20,000,000 to  21,000,000...Done. 326.65 seconds have elapsed\n",
      "importing rows  21,000,000 to  22,000,000...Done. 342.56 seconds have elapsed\n",
      "importing rows  22,000,000 to  23,000,000...Done. 357.95 seconds have elapsed\n",
      "importing rows  23,000,000 to  24,000,000...Done. 373.55 seconds have elapsed\n",
      "importing rows  24,000,000 to  25,000,000...Done. 389.51 seconds have elapsed\n",
      "importing rows  25,000,000 to  26,000,000...Done. 404.81 seconds have elapsed\n",
      "importing rows  26,000,000 to  26,024,289...Done. 405.18 seconds have elapsed\n"
     ]
    }
   ],
   "source": [
    "# run the program\n",
    "extract_transform_load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
